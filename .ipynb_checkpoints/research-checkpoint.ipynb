{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# --------scalers\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# --------cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# -------- classification\n",
    "# *** Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# *** KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# *** Decision Tree; Random Forest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# *** Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# *** SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "# --------  metrics:\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "sp500Symbols = sorted(list(sp500['Symbol']))\n",
    "\n",
    "sp400 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_400_companies')[0]\n",
    "sp400Symbols = sorted(list(sp400['Ticker symbol']))[359:]\n",
    "\n",
    "sp600 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_600_companies')[1]\n",
    "sp600Symbols = sorted(list(sp600['Ticker symbol']))\n",
    "\n",
    "spSymbols = sp500Symbols + sp400Symbols + sp600Symbols\n",
    "\n",
    "testSymbols = ['ABC']\n",
    "currentDate = datetime.date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "הרכשת נתונים"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_financial_data(symbol):\n",
    "    driver = webdriver.Chrome(executable_path=\"../chromedriver\")\n",
    "    driver.implicitly_wait(10)\n",
    "    url = 'https://seekingalpha.com/symbol/' + symbol + '/income-statement'\n",
    "    driver.get(url)\n",
    "\n",
    "    # Getting a list of the dates\n",
    "    dates_row = driver.find_element_by_class_name('dates-row')\n",
    "    dates_list = dates_row.find_elements_by_tag_name(\"li\")\n",
    "    for i in range(len(dates_list)):\n",
    "        dates_list[i] = dates_list[i].get_attribute('innerHTML')\n",
    "    \n",
    "    elem = driver.find_element_by_id('financials-tab')\n",
    "    abs_html = elem.get_attribute('innerHTML')\n",
    "    \n",
    "    # Changing to YoY view\n",
    "    view_arrow = driver.find_elements_by_class_name('select2-selection__arrow')[1]\n",
    "    view_arrow.click()\n",
    "    yoy_button = WebDriverWait(driver, 10).until(EC.presence_of_element_located(\n",
    "        (By.XPATH, \"//li[contains(text(),'YoY Growth')]\")))\n",
    "    yoy_button.click()\n",
    "    \n",
    "    elem = driver.find_element_by_id('financials-tab')\n",
    "    yoy_html = elem.get_attribute('innerHTML')\n",
    "    \n",
    "    driver.quit()\n",
    "    return (pd.read_html(abs_html), pd.read_html(yoy_html), dates_list)\n",
    "\n",
    "def clean_income_statement(statement):\n",
    "    if (statement[-30:] == \"  Created with Highstock 6.1.4\"):\n",
    "        return statement[:len(statement) - 30]\n",
    "    else:\n",
    "        return statement\n",
    "\n",
    "def clean_data(df):\n",
    "    df.replace('-',np.nan,inplace=True)\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    df['Income Statement'] = df['Income Statement'].apply(clean_income_statement)\n",
    "    df.set_index('Income Statement', inplace=True)\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    return df[~df.index.duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in sp400Symbols:\n",
    "    abs_tables, yoy_tables, dates = get_financial_data(symbol)\n",
    "\n",
    "    # Taking care of Absolute tables\n",
    "    for table in abs_tables:\n",
    "        table.columns = dates\n",
    "        \n",
    "    abs_df = pd.concat(abs_tables)\n",
    "    \n",
    "    abs_df = clean_data(abs_df)\n",
    "    \n",
    "    abs_df.to_csv(f'Stocks_Data\\sp400\\{symbol}_Absolute.csv')\n",
    "    \n",
    "    # Taking care of YoY tables\n",
    "    for table in yoy_tables:\n",
    "        table.columns = dates\n",
    "        \n",
    "    yoy_df = pd.concat(yoy_tables)\n",
    "\n",
    "    yoy_df = clean_data(yoy_df)\n",
    "    \n",
    "    yoy_df.to_csv(f'Stocks_Data\\sp400\\{symbol}_YoY.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "טיפול בנתונים"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_df():\n",
    "    yoy_agg_df = pd.DataFrame()\n",
    "    abs_agg_df = pd.DataFrame()\n",
    "    agg_errors = []\n",
    "\n",
    "    for folder in os.listdir('Stocks_Data'):\n",
    "        folder_path = os.path.join('Stocks_Data',folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            symbol, view = file.split('_')\n",
    "            file_path = os.path.join(f'Stocks_Data\\{folder}',file)\n",
    "            next_df = pd.read_csv(file_path)\n",
    "            next_df.set_index('Income Statement',inplace=True)\n",
    "            next_df = next_df.T\n",
    "            next_df['Symbol'] = [symbol for x in next_df.index]\n",
    "            if view[:3] == 'YoY':\n",
    "                try:\n",
    "                    yoy_agg_df = pd.concat([yoy_agg_df, next_df.iloc[:-1]])\n",
    "                except:\n",
    "                    agg_errors.append(file)\n",
    "                    continue\n",
    "            else:\n",
    "                try:\n",
    "                    abs_agg_df = pd.concat([abs_agg_df, next_df.iloc[:-1]])\n",
    "                except:\n",
    "                    agg_errors.append(file)\n",
    "                    continue\n",
    "    \n",
    "    abs_agg_df.index.name = 'Date'\n",
    "    yoy_agg_df.index.name = 'Date'\n",
    "    \n",
    "    return abs_agg_df, yoy_agg_df, agg_errors\n",
    "\n",
    "def check_prices(df :pd.DataFrame):\n",
    "    df[['Price Before', 'Price After','Change']] = np.nan\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    prices_errors = []\n",
    "    symbol = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            if symbol != row['Symbol']:\n",
    "                print(row['Symbol'], end=\"\\r\")\n",
    "                symbol = row['Symbol']\n",
    "                start_date = row['Date']\n",
    "                stock_data = web.DataReader(symbol, 'yahoo', start_date, currentDate)\n",
    "                stock_data_avg = stock_data.groupby(pd.Grouper(freq='MS'))['Close'].mean()\n",
    "            price_before = stock_data_avg.loc[row['Date']]\n",
    "            df.loc[index, 'Price Before'] = price_before\n",
    "            price_after = stock_data_avg.loc[row['Date'] + pd.DateOffset(months=1)]\n",
    "            df.loc[index, 'Price After'] = price_after\n",
    "            if price_after > price_before * 1.01:\n",
    "                df.loc[index, 'Change'] = 1\n",
    "            else:\n",
    "                df.loc[index, 'Change'] = 0\n",
    "        except:\n",
    "            prices_errors.append(row['Symbol'])\n",
    "            continue\n",
    "            \n",
    "    df.set_index('Date',inplace=True)\n",
    "    return df, prices_errors\n",
    "\n",
    "def cols_to_numeric(df :pd.DataFrame):\n",
    "    for col in df.columns:\n",
    "        if col == \"Symbol\":\n",
    "            continue\n",
    "        df.loc[:,col] = pd.to_numeric(df[col])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abs_agg_df, yoy_agg_df, agg_errors = agg_df()\n",
    "yoy_agg_df, prices_errors = check_prices(yoy_agg_df)\n",
    "yoy_agg_df = yoy_agg_df[~pd.isna(yoy_agg_df['Change'])]\n",
    "\n",
    "abs_agg_df.to_csv('Absolute_Aggregate.csv')\n",
    "yoy_agg_df.to_csv('YoY_Aggregate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agg_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (8,15,24,38,40,41,56,62,68,76,82,83,88,93,94,102,103,106,123,129,131,134) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "abs_agg_df = pd.read_csv('Absolute_Aggregate.csv')\n",
    "abs_agg_df.set_index('Date',inplace=True)\n",
    "\n",
    "yoy_agg_df = pd.read_csv('YoY_Aggregate.csv')\n",
    "yoy_agg_df.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_agg_df_filtered = abs_agg_df.dropna(axis = 1,thresh=2250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4379: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "# Dealing with minus values, cutting off the bracekts and adding \"-\" sign\n",
    "abs_agg_df_filtered.replace(r'\\(\\$?(\\d*,?\\d*\\.?\\d*)\\)', r'-\\1',regex=True, inplace=True)\n",
    "# Cutting off '$', '%' and ',' signs\n",
    "abs_agg_df_filtered.replace(r'\\$|\\%|\\,', '',regex=True, inplace=True)\n",
    "# Changing \"NM\" to \"0\"\n",
    "abs_agg_df_filtered.replace(r'^NM$', '0',regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "cols_to_numeric(abs_agg_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4511 entries, Dec 2016 to Dec 2020\n",
      "Data columns (total 38 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   Total Revenues                          4509 non-null   float64\n",
      " 1   Total Operating Expenses                4221 non-null   float64\n",
      " 2   Operating Income                        4226 non-null   float64\n",
      " 3   Net Interest Expenses                   3936 non-null   float64\n",
      " 4   EBT, Excl. Unusual Items                4511 non-null   float64\n",
      " 5   EBT, Incl. Unusual Items                4511 non-null   float64\n",
      " 6   Income Tax Expense                      4412 non-null   float64\n",
      " 7   Earnings From Continuing Operations     4511 non-null   float64\n",
      " 8   Net Income to Company                   4511 non-null   float64\n",
      " 9   Net Income                              4511 non-null   float64\n",
      " 10  NI to Common Incl Extra Items           4226 non-null   float64\n",
      " 11  NI to Common Excl. Extra Items          4226 non-null   float64\n",
      " 12  Revenue Per Share                       4497 non-null   float64\n",
      " 13  Basic EPS                               4499 non-null   float64\n",
      " 14  Basic EPS - Continuing Ops              4499 non-null   float64\n",
      " 15  Basic Weighted Average Shares Outst.    4499 non-null   float64\n",
      " 16  Diluted EPS                             4499 non-null   float64\n",
      " 17  Diluted EPS - Continuing Ops            4499 non-null   float64\n",
      " 18  Diluted Weighted Average Shares Outst.  4499 non-null   float64\n",
      " 19  Normalized Basic EPS                    4499 non-null   float64\n",
      " 20  Normalized Diluted EPS                  4499 non-null   float64\n",
      " 21  Dividend Per Share                      3310 non-null   float64\n",
      " 22  Payout Ratio                            3376 non-null   float64\n",
      " 23  EBITDA                                  4162 non-null   float64\n",
      " 24  EBITA                                   4171 non-null   float64\n",
      " 25  EBIT                                    4171 non-null   float64\n",
      " 26  EBITDAR                                 3397 non-null   float64\n",
      " 27  Effective Tax Rate                      4412 non-null   float64\n",
      " 28  Normalized Net Income                   4511 non-null   float64\n",
      " 29  Symbol                                  4511 non-null   object \n",
      " 30  Revenues                                3633 non-null   float64\n",
      " 31  Cost Of Revenues                        3398 non-null   float64\n",
      " 32  Gross Profit                            3419 non-null   float64\n",
      " 33  Selling General & Admin Expenses        3515 non-null   float64\n",
      " 34  Interest Expense                        3204 non-null   float64\n",
      " 35  Foreign Sales                           2908 non-null   float64\n",
      " 36  Interest And Investment Income          2351 non-null   float64\n",
      " 37  Other Non Operating Income (Expenses)   2513 non-null   float64\n",
      "dtypes: float64(37), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "abs_agg_df_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoy_agg_df_filtered = yoy_agg_df.dropna(axis = 1,thresh=3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4379: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "yoy_agg_df_filtered.replace(r'\\$|\\%|\\,', '',regex=True, inplace=True)\n",
    "yoy_agg_df_filtered.replace(r'\\(\\$?(\\d*,?\\d*\\.?\\d*)\\)', r'-\\1',regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "cols_to_numeric(yoy_agg_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3568 entries, 2017-12-01 to 2020-12-01\n",
      "Data columns (total 16 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   Total Revenues                          3565 non-null   float64\n",
      " 1   Total Operating Expenses                3335 non-null   float64\n",
      " 2   Operating Income                        3202 non-null   float64\n",
      " 3   EBT, Excl. Unusual Items                3377 non-null   float64\n",
      " 4   EBT, Incl. Unusual Items                3201 non-null   float64\n",
      " 5   Revenue Per Share                       3563 non-null   float64\n",
      " 6   Basic Weighted Average Shares Outst.    3566 non-null   float64\n",
      " 7   Diluted Weighted Average Shares Outst.  3566 non-null   float64\n",
      " 8   Normalized Basic EPS                    3368 non-null   float64\n",
      " 9   Normalized Diluted EPS                  3368 non-null   float64\n",
      " 10  EBITDA                                  3223 non-null   float64\n",
      " 11  Normalized Net Income                   3370 non-null   float64\n",
      " 12  Symbol                                  3568 non-null   object \n",
      " 13  Price Before                            3568 non-null   float64\n",
      " 14  Price After                             3568 non-null   float64\n",
      " 15  Change                                  3568 non-null   float64\n",
      "dtypes: float64(15), object(1)\n",
      "memory usage: 473.9+ KB\n"
     ]
    }
   ],
   "source": [
    "yoy_agg_df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_yoy_agg_df = yoy_agg_df_filtered.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2858 entries, 2017-12-01 to 2020-12-01\n",
      "Data columns (total 16 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   Total Revenues                          2858 non-null   float64\n",
      " 1   Total Operating Expenses                2858 non-null   float64\n",
      " 2   Operating Income                        2858 non-null   float64\n",
      " 3   EBT, Excl. Unusual Items                2858 non-null   float64\n",
      " 4   EBT, Incl. Unusual Items                2858 non-null   float64\n",
      " 5   Revenue Per Share                       2858 non-null   float64\n",
      " 6   Basic Weighted Average Shares Outst.    2858 non-null   float64\n",
      " 7   Diluted Weighted Average Shares Outst.  2858 non-null   float64\n",
      " 8   Normalized Basic EPS                    2858 non-null   float64\n",
      " 9   Normalized Diluted EPS                  2858 non-null   float64\n",
      " 10  EBITDA                                  2858 non-null   float64\n",
      " 11  Normalized Net Income                   2858 non-null   float64\n",
      " 12  Symbol                                  2858 non-null   object \n",
      " 13  Price Before                            2858 non-null   float64\n",
      " 14  Price After                             2858 non-null   float64\n",
      " 15  Change                                  2858 non-null   float64\n",
      "dtypes: float64(15), object(1)\n",
      "memory usage: 379.6+ KB\n"
     ]
    }
   ],
   "source": [
    "final_yoy_agg_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(df :pd.DataFrame, label_column :str, non_relevant_cols :list):\n",
    "    TRAINING_FEATURES = df.columns[df.columns != label_column]\n",
    "    TARGET_FEATURE = label_column\n",
    "\n",
    "    X = df[TRAINING_FEATURES]\n",
    "    X.drop(non_relevant_cols, axis=1, inplace=True)\n",
    "    y = df[TARGET_FEATURE]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "X, y = load_dataset(final_yoy_agg_df, 'Change', ['Symbol', 'Price Before', 'Price After'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Revenues</th>\n",
       "      <th>Total Operating Expenses</th>\n",
       "      <th>Operating Income</th>\n",
       "      <th>EBT, Excl. Unusual Items</th>\n",
       "      <th>EBT, Incl. Unusual Items</th>\n",
       "      <th>Revenue Per Share</th>\n",
       "      <th>Basic Weighted Average Shares Outst.</th>\n",
       "      <th>Diluted Weighted Average Shares Outst.</th>\n",
       "      <th>Normalized Basic EPS</th>\n",
       "      <th>Normalized Diluted EPS</th>\n",
       "      <th>EBITDA</th>\n",
       "      <th>Normalized Net Income</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>14.47</td>\n",
       "      <td>12.96</td>\n",
       "      <td>23.49</td>\n",
       "      <td>-10.56</td>\n",
       "      <td>-23.60</td>\n",
       "      <td>13.68</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-13.81</td>\n",
       "      <td>-14.78</td>\n",
       "      <td>25.72</td>\n",
       "      <td>-13.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>2.02</td>\n",
       "      <td>-25.50</td>\n",
       "      <td>-6.26</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>23.36</td>\n",
       "      <td>12.25</td>\n",
       "      <td>-9.12</td>\n",
       "      <td>-9.23</td>\n",
       "      <td>4.78</td>\n",
       "      <td>4.87</td>\n",
       "      <td>-4.60</td>\n",
       "      <td>-4.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-01</th>\n",
       "      <td>6.04</td>\n",
       "      <td>2.02</td>\n",
       "      <td>11.52</td>\n",
       "      <td>11.07</td>\n",
       "      <td>17.10</td>\n",
       "      <td>8.22</td>\n",
       "      <td>-2.01</td>\n",
       "      <td>-1.92</td>\n",
       "      <td>13.36</td>\n",
       "      <td>13.10</td>\n",
       "      <td>11.30</td>\n",
       "      <td>11.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>5.44</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>15.22</td>\n",
       "      <td>7.00</td>\n",
       "      <td>15.22</td>\n",
       "      <td>8.43</td>\n",
       "      <td>-2.76</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>10.08</td>\n",
       "      <td>9.91</td>\n",
       "      <td>14.73</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>3.22</td>\n",
       "      <td>-9.83</td>\n",
       "      <td>7.63</td>\n",
       "      <td>8.65</td>\n",
       "      <td>8.94</td>\n",
       "      <td>5.42</td>\n",
       "      <td>-2.11</td>\n",
       "      <td>-2.20</td>\n",
       "      <td>10.41</td>\n",
       "      <td>10.55</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>-0.44</td>\n",
       "      <td>15.63</td>\n",
       "      <td>-19.71</td>\n",
       "      <td>-18.40</td>\n",
       "      <td>-15.85</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-18.60</td>\n",
       "      <td>-17.63</td>\n",
       "      <td>-11.90</td>\n",
       "      <td>-18.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>-21.24</td>\n",
       "      <td>-41.24</td>\n",
       "      <td>-11.07</td>\n",
       "      <td>6.19</td>\n",
       "      <td>-48.09</td>\n",
       "      <td>-19.44</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>9.01</td>\n",
       "      <td>9.01</td>\n",
       "      <td>-2.64</td>\n",
       "      <td>6.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01</th>\n",
       "      <td>-13.41</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>-86.92</td>\n",
       "      <td>-87.47</td>\n",
       "      <td>-98.53</td>\n",
       "      <td>-11.61</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-2.05</td>\n",
       "      <td>-87.22</td>\n",
       "      <td>-87.23</td>\n",
       "      <td>-63.37</td>\n",
       "      <td>-87.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>8.21</td>\n",
       "      <td>11.06</td>\n",
       "      <td>-5.05</td>\n",
       "      <td>-2.86</td>\n",
       "      <td>0.19</td>\n",
       "      <td>16.79</td>\n",
       "      <td>-7.34</td>\n",
       "      <td>-7.73</td>\n",
       "      <td>4.44</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>6.30</td>\n",
       "      <td>1.45</td>\n",
       "      <td>16.42</td>\n",
       "      <td>20.85</td>\n",
       "      <td>25.06</td>\n",
       "      <td>6.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.89</td>\n",
       "      <td>13.78</td>\n",
       "      <td>14.50</td>\n",
       "      <td>13.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2286 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total Revenues  Total Operating Expenses  Operating Income  \\\n",
       "Date                                                                     \n",
       "2017-12-01           14.47                     12.96             23.49   \n",
       "2019-02-01            2.02                    -25.50             -6.26   \n",
       "2017-06-01            6.04                      2.02             11.52   \n",
       "2017-12-01            5.44                     -2.37             15.22   \n",
       "2018-12-01            3.22                     -9.83              7.63   \n",
       "...                    ...                       ...               ...   \n",
       "2018-03-01           -0.44                     15.63            -19.71   \n",
       "2019-12-01          -21.24                    -41.24            -11.07   \n",
       "2021-01-01          -13.41                    -13.69            -86.92   \n",
       "2019-12-01            8.21                     11.06             -5.05   \n",
       "2019-12-01            6.30                      1.45             16.42   \n",
       "\n",
       "            EBT, Excl. Unusual Items  EBT, Incl. Unusual Items  \\\n",
       "Date                                                             \n",
       "2017-12-01                    -10.56                    -23.60   \n",
       "2019-02-01                     -4.82                     23.36   \n",
       "2017-06-01                     11.07                     17.10   \n",
       "2017-12-01                      7.00                     15.22   \n",
       "2018-12-01                      8.65                      8.94   \n",
       "...                              ...                       ...   \n",
       "2018-03-01                    -18.40                    -15.85   \n",
       "2019-12-01                      6.19                    -48.09   \n",
       "2021-01-01                    -87.47                    -98.53   \n",
       "2019-12-01                     -2.86                      0.19   \n",
       "2019-12-01                     20.85                     25.06   \n",
       "\n",
       "            Revenue Per Share  Basic Weighted Average Shares Outst.  \\\n",
       "Date                                                                  \n",
       "2017-12-01              13.68                                  0.73   \n",
       "2019-02-01              12.25                                 -9.12   \n",
       "2017-06-01               8.22                                 -2.01   \n",
       "2017-12-01               8.43                                 -2.76   \n",
       "2018-12-01               5.42                                 -2.11   \n",
       "...                       ...                                   ...   \n",
       "2018-03-01              -0.56                                  0.12   \n",
       "2019-12-01             -19.44                                 -2.22   \n",
       "2021-01-01             -11.61                                 -2.02   \n",
       "2019-12-01              16.79                                 -7.34   \n",
       "2019-12-01               6.32                                  0.00   \n",
       "\n",
       "            Diluted Weighted Average Shares Outst.  Normalized Basic EPS  \\\n",
       "Date                                                                       \n",
       "2017-12-01                                    0.96                -13.81   \n",
       "2019-02-01                                   -9.23                  4.78   \n",
       "2017-06-01                                   -1.92                 13.36   \n",
       "2017-12-01                                   -2.73                 10.08   \n",
       "2018-12-01                                   -2.20                 10.41   \n",
       "...                                            ...                   ...   \n",
       "2018-03-01                                   -0.92                -18.60   \n",
       "2019-12-01                                   -2.22                  9.01   \n",
       "2021-01-01                                   -2.05                -87.22   \n",
       "2019-12-01                                   -7.73                  4.44   \n",
       "2019-12-01                                    0.00                 13.89   \n",
       "\n",
       "            Normalized Diluted EPS  EBITDA  Normalized Net Income  \n",
       "Date                                                               \n",
       "2017-12-01                  -14.78   25.72                 -13.51  \n",
       "2019-02-01                    4.87   -4.60                  -4.82  \n",
       "2017-06-01                   13.10   11.30                  11.07  \n",
       "2017-12-01                    9.91   14.73                   7.00  \n",
       "2018-12-01                   10.55    8.05                   8.06  \n",
       "...                            ...     ...                    ...  \n",
       "2018-03-01                  -17.63  -11.90                 -18.40  \n",
       "2019-12-01                    9.01   -2.64                   6.19  \n",
       "2021-01-01                  -87.23  -63.37                 -87.47  \n",
       "2019-12-01                    5.16    0.06                  -2.86  \n",
       "2019-12-01                   13.78   14.50                  13.86  \n",
       "\n",
       "[2286 rows x 12 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = X[X.index <= '2019-12-31']\n",
    "new_X_test = X[X.index > '2019-12-31']\n",
    "new_y_train = y[y.index <= '2019-12-31']\n",
    "new_y_test = y[y.index > '2019-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Revenues</th>\n",
       "      <th>Total Operating Expenses</th>\n",
       "      <th>Operating Income</th>\n",
       "      <th>EBT, Excl. Unusual Items</th>\n",
       "      <th>EBT, Incl. Unusual Items</th>\n",
       "      <th>Revenue Per Share</th>\n",
       "      <th>Basic Weighted Average Shares Outst.</th>\n",
       "      <th>Diluted Weighted Average Shares Outst.</th>\n",
       "      <th>Normalized Basic EPS</th>\n",
       "      <th>Normalized Diluted EPS</th>\n",
       "      <th>EBITDA</th>\n",
       "      <th>Normalized Net Income</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>1.28</td>\n",
       "      <td>4.23</td>\n",
       "      <td>-8.80</td>\n",
       "      <td>-6.85</td>\n",
       "      <td>-30.13</td>\n",
       "      <td>-3.23</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.60</td>\n",
       "      <td>-9.67</td>\n",
       "      <td>-10.25</td>\n",
       "      <td>1.27</td>\n",
       "      <td>-6.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>10.59</td>\n",
       "      <td>12.06</td>\n",
       "      <td>4.87</td>\n",
       "      <td>-22.06</td>\n",
       "      <td>70.94</td>\n",
       "      <td>9.30</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.26</td>\n",
       "      <td>-25.90</td>\n",
       "      <td>-24.59</td>\n",
       "      <td>9.10</td>\n",
       "      <td>-24.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>7.07</td>\n",
       "      <td>4.98</td>\n",
       "      <td>15.78</td>\n",
       "      <td>18.46</td>\n",
       "      <td>-27.38</td>\n",
       "      <td>6.66</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.41</td>\n",
       "      <td>19.94</td>\n",
       "      <td>19.08</td>\n",
       "      <td>8.93</td>\n",
       "      <td>19.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>0.90</td>\n",
       "      <td>5.85</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>618.55</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.27</td>\n",
       "      <td>-3.32</td>\n",
       "      <td>-2.95</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>-32.85</td>\n",
       "      <td>-37.37</td>\n",
       "      <td>-30.22</td>\n",
       "      <td>-53.79</td>\n",
       "      <td>-71.54</td>\n",
       "      <td>-33.11</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-54.32</td>\n",
       "      <td>-54.14</td>\n",
       "      <td>-33.57</td>\n",
       "      <td>-54.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>13.33</td>\n",
       "      <td>1.29</td>\n",
       "      <td>65.46</td>\n",
       "      <td>148.64</td>\n",
       "      <td>495.45</td>\n",
       "      <td>12.12</td>\n",
       "      <td>1.07</td>\n",
       "      <td>1.14</td>\n",
       "      <td>146.30</td>\n",
       "      <td>145.94</td>\n",
       "      <td>25.50</td>\n",
       "      <td>148.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>6.33</td>\n",
       "      <td>2.76</td>\n",
       "      <td>12.77</td>\n",
       "      <td>14.44</td>\n",
       "      <td>14.12</td>\n",
       "      <td>5.54</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.54</td>\n",
       "      <td>13.58</td>\n",
       "      <td>13.75</td>\n",
       "      <td>10.04</td>\n",
       "      <td>14.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>8.57</td>\n",
       "      <td>1.18</td>\n",
       "      <td>19.40</td>\n",
       "      <td>21.80</td>\n",
       "      <td>24.19</td>\n",
       "      <td>9.86</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>23.13</td>\n",
       "      <td>23.07</td>\n",
       "      <td>15.91</td>\n",
       "      <td>21.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>9.76</td>\n",
       "      <td>12.96</td>\n",
       "      <td>8.18</td>\n",
       "      <td>8.92</td>\n",
       "      <td>10.82</td>\n",
       "      <td>11.34</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>10.48</td>\n",
       "      <td>10.72</td>\n",
       "      <td>10.63</td>\n",
       "      <td>9.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>7.47</td>\n",
       "      <td>10.68</td>\n",
       "      <td>10.89</td>\n",
       "      <td>11.75</td>\n",
       "      <td>6.57</td>\n",
       "      <td>8.56</td>\n",
       "      <td>-1.02</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>12.52</td>\n",
       "      <td>12.68</td>\n",
       "      <td>14.06</td>\n",
       "      <td>11.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2164 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total Revenues  Total Operating Expenses  Operating Income  \\\n",
       "Date                                                                     \n",
       "2017-12-01            1.28                      4.23             -8.80   \n",
       "2018-12-01           10.59                     12.06              4.87   \n",
       "2019-12-01            7.07                      4.98             15.78   \n",
       "2017-12-01            0.90                      5.85             -1.63   \n",
       "2018-12-01          -32.85                    -37.37            -30.22   \n",
       "...                    ...                       ...               ...   \n",
       "2018-12-01           13.33                      1.29             65.46   \n",
       "2019-12-01            6.33                      2.76             12.77   \n",
       "2017-12-01            8.57                      1.18             19.40   \n",
       "2018-12-01            9.76                     12.96              8.18   \n",
       "2019-12-01            7.47                     10.68             10.89   \n",
       "\n",
       "            EBT, Excl. Unusual Items  EBT, Incl. Unusual Items  \\\n",
       "Date                                                             \n",
       "2017-12-01                     -6.85                    -30.13   \n",
       "2018-12-01                    -22.06                     70.94   \n",
       "2019-12-01                     18.46                    -27.38   \n",
       "2017-12-01                     -0.93                    618.55   \n",
       "2018-12-01                    -53.79                    -71.54   \n",
       "...                              ...                       ...   \n",
       "2018-12-01                    148.64                    495.45   \n",
       "2019-12-01                     14.44                     14.12   \n",
       "2017-12-01                     21.80                     24.19   \n",
       "2018-12-01                      8.92                     10.82   \n",
       "2019-12-01                     11.75                      6.57   \n",
       "\n",
       "            Revenue Per Share  Basic Weighted Average Shares Outst.  \\\n",
       "Date                                                                  \n",
       "2017-12-01              -3.23                                  4.58   \n",
       "2018-12-01               9.30                                  1.24   \n",
       "2019-12-01               6.66                                  0.35   \n",
       "2017-12-01              -0.55                                  1.46   \n",
       "2018-12-01             -33.11                                  0.39   \n",
       "...                       ...                                   ...   \n",
       "2018-12-01              12.12                                  1.07   \n",
       "2019-12-01               5.54                                  0.75   \n",
       "2017-12-01               9.86                                 -1.17   \n",
       "2018-12-01              11.34                                 -1.40   \n",
       "2019-12-01               8.56                                 -1.02   \n",
       "\n",
       "            Diluted Weighted Average Shares Outst.  Normalized Basic EPS  \\\n",
       "Date                                                                       \n",
       "2017-12-01                                    4.60                 -9.67   \n",
       "2018-12-01                                    1.26                -25.90   \n",
       "2019-12-01                                    0.41                 19.94   \n",
       "2017-12-01                                    1.27                 -3.32   \n",
       "2018-12-01                                    0.41                -54.32   \n",
       "...                                            ...                   ...   \n",
       "2018-12-01                                    1.14                146.30   \n",
       "2019-12-01                                    0.54                 13.58   \n",
       "2017-12-01                                   -1.02                 23.13   \n",
       "2018-12-01                                   -1.27                 10.48   \n",
       "2019-12-01                                   -1.05                 12.52   \n",
       "\n",
       "            Normalized Diluted EPS  EBITDA  Normalized Net Income  \n",
       "Date                                                               \n",
       "2017-12-01                  -10.25    1.27                  -6.23  \n",
       "2018-12-01                  -24.59    9.10                 -24.17  \n",
       "2019-12-01                   19.08    8.93                  19.89  \n",
       "2017-12-01                   -2.95    0.11                  -1.95  \n",
       "2018-12-01                  -54.14  -33.57                 -54.02  \n",
       "...                            ...     ...                    ...  \n",
       "2018-12-01                  145.94   25.50                 148.64  \n",
       "2019-12-01                   13.75   10.04                  14.44  \n",
       "2017-12-01                   23.07   15.91                  21.75  \n",
       "2018-12-01                   10.72   10.63                   9.10  \n",
       "2019-12-01                   12.68   14.06                  11.34  \n",
       "\n",
       "[2164 rows x 12 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train, X_test, scale_type):\n",
    "    X_train_scaled = pd.DataFrame()\n",
    "    if scale_type == 'minmax':\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.fit_transform(X_test)\n",
    "    elif scale_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.fit_transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - scaling\n",
    "X_train_minmax_scaled, X_test_minmax_scaled = scale_features(X_train, X_test, 'minmax')\n",
    "X_train_standard_scaled, X_test_standard_scaled = scale_features(X_train, X_test, 'standard')\n",
    "\n",
    "# Split by date - scaling\n",
    "new_X_train_minmax_scaled, new_X_test_minmax_scaled = scale_features(new_X_train, new_X_test, 'minmax')\n",
    "new_X_train_standard_scaled, new_X_test_standard_scaled = scale_features(new_X_train, new_X_test, 'standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Random split - test\n",
    "LR_classification_model = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = LR_classification_model.predict(X_test)\n",
    "\n",
    "# Split by date - test\n",
    "LR_classification_model = LogisticRegression().fit(new_X_train, new_y_train)\n",
    "new_y_pred = LR_classification_model.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.6136363636363636\n",
      "precision is: 0.6223021582733813\n",
      "recall is: 0.969187675070028\n",
      "f1 is: 0.7579408543263966\n",
      "[[  5 210]\n",
      " [ 11 346]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.5345821325648416\n",
      "precision is: 0.582010582010582\n",
      "recall is: 0.7932692307692307\n",
      "f1 is: 0.6714140386571719\n",
      "[[ 41 237]\n",
      " [ 86 330]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mimax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "LR_classification_minmax_model = LogisticRegression().fit(X_train_minmax_scaled, y_train)\n",
    "y_pred = LR_classification_minmax_model.predict(X_test_minmax_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "LR_classification_minmax_model = LogisticRegression().fit(new_X_train_minmax_scaled, new_y_train)\n",
    "new_y_pred = LR_classification_minmax_model.predict(new_X_test_minmax_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.6188811188811189\n",
      "precision is: 0.6221441124780316\n",
      "recall is: 0.9915966386554622\n",
      "f1 is: 0.7645788336933045\n",
      "[[  0 215]\n",
      " [  3 354]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.590778097982709\n",
      "precision is: 0.5973451327433629\n",
      "recall is: 0.9735576923076923\n",
      "f1 is: 0.7404021937842779\n",
      "[[  5 273]\n",
      " [ 11 405]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "LR_classification_standard_model = LogisticRegression().fit(X_train_standard_scaled, y_train)\n",
    "y_pred = LR_classification_standard_model.predict(X_test_standard_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "LR_classification_standard_model = LogisticRegression().fit(new_X_train_standard_scaled, new_y_train)\n",
    "new_y_pred = LR_classification_standard_model.predict(new_X_test_standard_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.5979020979020979\n",
      "precision is: 0.6169429097605893\n",
      "recall is: 0.938375350140056\n",
      "f1 is: 0.7444444444444445\n",
      "[[  7 208]\n",
      " [ 22 335]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.5677233429394812\n",
      "precision is: 0.5914826498422713\n",
      "recall is: 0.9014423076923077\n",
      "f1 is: 0.7142857142857142\n",
      "[[ 19 259]\n",
      " [ 41 375]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k_for_KNN(X_train, y_train):\n",
    "    parameters = {'n_neighbors':range(3,16,2)}\n",
    "    knn = KNeighborsClassifier()\n",
    "    clf = GridSearchCV(knn, parameters,scoring=make_scorer(metrics.precision_score))\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_K = clf.best_params_['n_neighbors']\n",
    "    \n",
    "    return clf, best_K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "KNN_classification_model, best_K = find_best_k_for_KNN(X_train, y_train)\n",
    "y_pred = KNN_classification_model.predict(X_test)\n",
    "\n",
    "# Split by date - test\n",
    "KNN_classification_model, new_best_K = find_best_k_for_KNN(new_X_train, new_y_train)\n",
    "new_y_pred = KNN_classification_model.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best K is: 9\n",
      "accuracy is: 0.5716783216783217\n",
      "precision is: 0.6465968586387435\n",
      "recall is: 0.6918767507002801\n",
      "f1 is: 0.6684709066305818\n",
      "[[ 80 135]\n",
      " [110 247]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"best K is:\",best_K)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best K is: 11\n",
      "accuracy is: 0.537463976945245\n",
      "precision is: 0.6004228329809725\n",
      "recall is: 0.6826923076923077\n",
      "f1 is: 0.6389201349831273\n",
      "[[ 89 189]\n",
      " [132 284]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"best K is:\",new_best_K)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mimax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "KNN_classification_minmax_model, best_K = find_best_k_for_KNN(X_train_minmax_scaled, y_train)\n",
    "y_pred = KNN_classification_minmax_model.predict(X_test_minmax_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "KNN_classification_model, new_best_K = find_best_k_for_KNN(new_X_train_minmax_scaled, new_y_train)\n",
    "new_y_pred = KNN_classification_model.predict(new_X_test_minmax_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best K is: 13\n",
      "accuracy is: 0.6171328671328671\n",
      "precision is: 0.6214788732394366\n",
      "recall is: 0.988795518207283\n",
      "f1 is: 0.7632432432432433\n",
      "[[  0 215]\n",
      " [  4 353]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"best K is:\",best_K)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best K is: 13\n",
      "accuracy is: 0.5951008645533141\n",
      "precision is: 0.5982532751091703\n",
      "recall is: 0.9879807692307693\n",
      "f1 is: 0.7452402538531279\n",
      "[[  2 276]\n",
      " [  5 411]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"best K is:\",new_best_K)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "KNN_classification_standard_model, best_K = find_best_k_for_KNN(X_train_standard_scaled, y_train)\n",
    "y_pred = KNN_classification_standard_model.predict(X_test_standard_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "KNN_classification_model, new_best_K = find_best_k_for_KNN(new_X_train_standard_scaled, new_y_train)\n",
    "new_y_pred = KNN_classification_model.predict(new_X_test_standard_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best K is: 15\n",
      "accuracy is: 0.6171328671328671\n",
      "precision is: 0.6468085106382979\n",
      "recall is: 0.8515406162464986\n",
      "f1 is: 0.7351874244256349\n",
      "[[ 49 166]\n",
      " [ 53 304]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"best K is:\",best_K)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best K is: 9\n",
      "accuracy is: 0.5590778097982709\n",
      "precision is: 0.6041666666666666\n",
      "recall is: 0.7668269230769231\n",
      "f1 is: 0.6758474576271186\n",
      "[[ 69 209]\n",
      " [ 97 319]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"best K is:\",new_best_K)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_decision_tree_params(X_train, y_train):\n",
    "    parameters = {'max_depth':range(2,11), 'min_samples_split':range(5,21)}\n",
    "    dt = DecisionTreeClassifier()\n",
    "    clf = GridSearchCV(dt, parameters,scoring=make_scorer(metrics.precision_score))\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_max_depth = clf.best_params_['max_depth']\n",
    "    best_min_samples_split = clf.best_params_['min_samples_split']\n",
    "    \n",
    "    return clf, best_max_depth, best_min_samples_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "DT_classification_model, best_max_depth, best_min_samples_split = find_best_decision_tree_params(X_train, y_train)\n",
    "y_pred = DT_classification_model.predict(X_test)\n",
    "\n",
    "# Split by date - test\n",
    "DT_classification_model, new_best_max_depth, new_best_min_samples_split = find_best_decision_tree_params(new_X_train, new_y_train)\n",
    "new_y_pred = DT_classification_model.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max depth is: 8\n",
      "best min samples split is: 14\n",
      "accuracy is: 0.5821678321678322\n",
      "precision is: 0.6520618556701031\n",
      "recall is: 0.7086834733893558\n",
      "f1 is: 0.6791946308724832\n",
      "[[ 80 135]\n",
      " [104 253]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"best max depth is:\",best_max_depth)\n",
    "print(\"best min samples split is:\",best_min_samples_split)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max depth is: 5\n",
      "best min samples split is: 15\n",
      "accuracy is: 0.5244956772334294\n",
      "precision is: 0.583984375\n",
      "recall is: 0.71875\n",
      "f1 is: 0.6443965517241379\n",
      "[[ 65 213]\n",
      " [117 299]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"best max depth is:\",new_best_max_depth)\n",
    "print(\"best min samples split is:\",new_best_min_samples_split)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mimax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "DT_classification_minmax_model, best_max_depth, best_min_samples_split = find_best_decision_tree_params(X_train_minmax_scaled, y_train)\n",
    "y_pred = DT_classification_minmax_model.predict(X_test_minmax_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "DT_classification_minmax_model, new_best_max_depth, best_min_samples_split = find_best_decision_tree_params(new_X_train_minmax_scaled, new_y_train)\n",
    "new_y_pred = DT_classification_minmax_model.predict(new_X_test_minmax_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max depth is: 8\n",
      "best min samples split is: 14\n",
      "accuracy is: 0.6223776223776224\n",
      "precision is: 0.6239015817223199\n",
      "recall is: 0.9943977591036415\n",
      "f1 is: 0.7667386609071275\n",
      "[[  1 214]\n",
      " [  2 355]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"best max depth is:\",best_max_depth)\n",
    "print(\"best min samples split is:\",best_min_samples_split)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max depth is: 5\n",
      "best min samples split is: 15\n",
      "accuracy is: 0.4020172910662824\n",
      "precision is: 0.6666666666666666\n",
      "recall is: 0.004807692307692308\n",
      "f1 is: 0.00954653937947494\n",
      "[[277   1]\n",
      " [414   2]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"best max depth is:\",new_best_max_depth)\n",
    "print(\"best min samples split is:\",new_best_min_samples_split)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "DT_classification_standard_model, best_max_depth, best_min_samples_split = find_best_decision_tree_params(X_train_standard_scaled, y_train)\n",
    "y_pred = DT_classification_standard_model.predict(X_test_standard_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "DT_classification_standard_model, new_best_max_depth, best_min_samples_split = find_best_decision_tree_params(new_X_train_standard_scaled, new_y_train)\n",
    "new_y_pred = DT_classification_standard_model.predict(new_X_test_standard_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max depth is: 8\n",
      "best min samples split is: 14\n",
      "accuracy is: 0.5944055944055944\n",
      "precision is: 0.6361655773420479\n",
      "recall is: 0.8179271708683473\n",
      "f1 is: 0.715686274509804\n",
      "[[ 48 167]\n",
      " [ 65 292]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"best max depth is:\",best_max_depth)\n",
    "print(\"best min samples split is:\",best_min_samples_split)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max depth is: 5\n",
      "best min samples split is: 15\n",
      "accuracy is: 0.4899135446685879\n",
      "precision is: 0.5654008438818565\n",
      "recall is: 0.6442307692307693\n",
      "f1 is: 0.6022471910112359\n",
      "[[ 72 206]\n",
      " [148 268]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"best max depth is:\",new_best_max_depth)\n",
    "print(\"best min samples split is:\",new_best_min_samples_split)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_random_forest_params(X_train, y_train):\n",
    "    parameters = {'n_estimators':range(50,551,100)}\n",
    "    rf = RandomForestClassifier()\n",
    "    clf = GridSearchCV(rf, parameters,scoring=make_scorer(metrics.precision_score))\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_n_estimators = clf.best_params_['n_estimators']\n",
    "    \n",
    "    return clf, best_n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "RF_classification_model, best_n_estimators = find_best_random_forest_params(X_train, y_train)\n",
    "y_pred = RF_classification_model.predict(X_test)\n",
    "\n",
    "# Split by date - test\n",
    "RF_classification_model, new_best_n_estimators = find_best_random_forest_params(new_X_train, new_y_train)\n",
    "new_y_pred = RF_classification_model.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of estimators is: 50\n",
      "accuracy is: 0.6031468531468531\n",
      "precision is: 0.6600985221674877\n",
      "recall is: 0.7507002801120448\n",
      "f1 is: 0.7024901703800787\n",
      "[[ 77 138]\n",
      " [ 89 268]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"best number of estimators is:\",best_n_estimators)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of estimators is: 150\n",
      "accuracy is: 0.5461095100864554\n",
      "precision is: 0.6124721603563474\n",
      "recall is: 0.6610576923076923\n",
      "f1 is: 0.6358381502890172\n",
      "[[104 174]\n",
      " [141 275]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"best number of estimators is:\",new_best_n_estimators)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mimax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "RF_classification_minmax_model, best_n_estimators = find_best_random_forest_params(X_train_minmax_scaled, y_train)\n",
    "y_pred = RF_classification_minmax_model.predict(X_test_minmax_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "RF_classification_minmax_model, new_best_n_estimators = find_best_random_forest_params(new_X_train_minmax_scaled, new_y_train)\n",
    "new_y_pred = RF_classification_minmax_model.predict(new_X_test_minmax_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of estimators is: 50\n",
      "accuracy is: 0.5891608391608392\n",
      "precision is: 0.6173076923076923\n",
      "recall is: 0.8991596638655462\n",
      "f1 is: 0.7320410490307868\n",
      "[[ 16 199]\n",
      " [ 36 321]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"best number of estimators is:\",best_n_estimators)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of estimators is: 50\n",
      "accuracy is: 0.40057636887608067\n",
      "precision is: 0.5\n",
      "recall is: 0.002403846153846154\n",
      "f1 is: 0.004784688995215312\n",
      "[[277   1]\n",
      " [415   1]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"best number of estimators is:\",new_best_n_estimators)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "RF_classification_standard_model, best_n_estimators = find_best_random_forest_params(X_train_standard_scaled, y_train)\n",
    "y_pred = RF_classification_standard_model.predict(X_test_standard_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "RF_classification_standard_model, new_best_n_estimators = find_best_random_forest_params(new_X_train_standard_scaled, new_y_train)\n",
    "new_y_pred = RF_classification_standard_model.predict(new_X_test_standard_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of estimators is: 350\n",
      "accuracy is: 0.6188811188811189\n",
      "precision is: 0.6349514563106796\n",
      "recall is: 0.9159663865546218\n",
      "f1 is: 0.75\n",
      "[[ 27 188]\n",
      " [ 30 327]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"best number of estimators is:\",best_n_estimators)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of estimators is: 250\n",
      "accuracy is: 0.5432276657060519\n",
      "precision is: 0.6078431372549019\n",
      "recall is: 0.6706730769230769\n",
      "f1 is: 0.6377142857142857\n",
      "[[ 98 180]\n",
      " [137 279]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"best number of estimators is:\",new_best_n_estimators)\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "NB_classification_model = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = NB_classification_model.predict(X_test)\n",
    "\n",
    "# Split by date - test\n",
    "NB_classification_model = GaussianNB().fit(new_X_train, new_y_train)\n",
    "new_y_pred = NB_classification_model.predict(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.6136363636363636\n",
      "precision is: 0.6273408239700374\n",
      "recall is: 0.938375350140056\n",
      "f1 is: 0.7519640852974185\n",
      "[[ 16 199]\n",
      " [ 22 335]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.5951008645533141\n",
      "precision is: 0.5988286969253295\n",
      "recall is: 0.9831730769230769\n",
      "f1 is: 0.7443130118289354\n",
      "[[  4 274]\n",
      " [  7 409]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mimax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "NB_classification_minmax_model = GaussianNB().fit(X_train_minmax_scaled, y_train)\n",
    "y_pred = NB_classification_minmax_model.predict(X_test_minmax_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "NB_classification_minmax_model = GaussianNB().fit(new_X_train_minmax_scaled, new_y_train)\n",
    "new_y_pred = NB_classification_minmax_model.predict(new_X_test_minmax_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.3758741258741259\n",
      "precision is: 0.5\n",
      "recall is: 0.0028011204481792717\n",
      "f1 is: 0.005571030640668524\n",
      "[[214   1]\n",
      " [356   1]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.40057636887608067\n",
      "precision is: 0.0\n",
      "recall is: 0.0\n",
      "f1 is: 0.0\n",
      "[[278   0]\n",
      " [416   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split - test\n",
    "NB_classification_standard_model = GaussianNB().fit(X_train_standard_scaled, y_train)\n",
    "y_pred = NB_classification_standard_model.predict(X_test_standard_scaled)\n",
    "\n",
    "# Split by date - test\n",
    "NB_classification_standard_model = GaussianNB().fit(new_X_train_standard_scaled, new_y_train)\n",
    "new_y_pred = NB_classification_standard_model.predict(new_X_test_standard_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.40384615384615385\n",
      "precision is: 0.6428571428571429\n",
      "recall is: 0.10084033613445378\n",
      "f1 is: 0.17433414043583537\n",
      "[[195  20]\n",
      " [321  36]]\n"
     ]
    }
   ],
   "source": [
    "# Random split - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.38760806916426516\n",
      "precision is: 0.47593582887700536\n",
      "recall is: 0.21394230769230768\n",
      "f1 is: 0.29519071310116085\n",
      "[[180  98]\n",
      " [327  89]]\n"
     ]
    }
   ],
   "source": [
    "# Split by date - result\n",
    "print(\"accuracy is:\",metrics.accuracy_score(new_y_test, new_y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(new_y_test, new_y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(new_y_test, new_y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(new_y_test, new_y_pred))\n",
    "print(metrics.confusion_matrix(new_y_test, new_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBClassifier(verbosity = 0, booster = 'gbtree', objective = 'binary:logistic', min_child_weight = 2,\n",
    "                           max_delta_step = 2 ,learning_rate = 0.9, n_estimators = 8, tree_method = 'hist',\n",
    "                           max_depth = 7)\n",
    "# dart\n",
    "# gbtree\n",
    "# gblinear\n",
    "\n",
    "xg_reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# xg_reg = xgb.XGBClassifier(verbosity = 0, booster = 'gbtree', objective = 'binary:logistic', min_child_weight = 1,\n",
    "#                            max_delta_step = 0.4 ,learning_rate = 0.9, n_estimators = 9, tree_method = 'hist',\n",
    "#                            max_depth = 6)\n",
    "# accuracy is: 0.585\n",
    "# precision is: 0.6745098039215687\n",
    "# recall is: 0.6745098039215687\n",
    "# f1 is: 0.6745098039215687\n",
    "# array([[ 62,  83],\n",
    "#        [ 83, 172]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"accuracy is:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision is:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"recall is:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"f1 is:\",metrics.f1_score(y_test, y_pred))\n",
    "metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# xg_reg = xgb.XGBClassifier(objective ='reg:pseudohubererror',min_child_weight = 1.1, max_delta_step = 1,\n",
    "#                            n_estimators = 9)\n",
    "# accuracy is: 0.6225\n",
    "# precision is: 0.6573426573426573\n",
    "# recall is: 0.7800829875518672\n",
    "# f1 is: 0.713472485768501\n",
    "# array([[ 61,  98],\n",
    "#        [ 53, 188]], dtype=int64)\n",
    "\n",
    "# xg_reg = xgb.XGBClassifier(objective ='reg:logistic',min_child_weight = 1.9, max_delta_step = 0.4, learning_rate = 0.9,\n",
    "#                 n_estimators = 15, tree_method = 'hist', max_depth = 10)\n",
    "# accuracy is: 0.625\n",
    "# precision is: 0.6666666666666666\n",
    "# recall is: 0.7551867219917012\n",
    "# f1 is: 0.708171206225681\n",
    "# array([[ 68,  91],\n",
    "#        [ 59, 182]], dtype=int64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
